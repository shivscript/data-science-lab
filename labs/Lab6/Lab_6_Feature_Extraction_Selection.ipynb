{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d034fee",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 6: Feature Extraction and Selection Techniques\n",
    "\n",
    "## Objectives\n",
    "1. Understand the importance of feature extraction and selection in Data Science.\n",
    "2. Experiment with encoding methods like one-hot encoding.\n",
    "3. Learn to create new features based on domain expertise.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201bb2d2",
   "metadata": {},
   "source": [
    "\n",
    "### What is Feature Extraction?\n",
    "Feature extraction involves transforming raw data into a format suitable for modeling. It includes methods like:\n",
    "- Encoding categorical variables\n",
    "- Creating new features from existing data\n",
    "\n",
    "### What is Feature Selection?\n",
    "Feature selection focuses on selecting the most relevant features for training models. This can help improve:\n",
    "- Model performance\n",
    "- Computational efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8a998-cbc1-47fd-9d5b-04d1cac4bfd3",
   "metadata": {},
   "source": [
    "## Differences Between Feature Extraction and Feature Selection\n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "### **Feature Extraction**\n",
    "- **Purpose**: Creates new features by transforming the original data.\n",
    "- **Approach**: Combines or derives features from the existing dataset using mathematical or statistical transformations.\n",
    "- **Outcome**: Generates a new feature space, often in reduced dimensions, while retaining important information.\n",
    "- **Examples**:\n",
    "  - Principal Component Analysis (PCA)\n",
    "  - Creating new features like BMI from weight and height.\n",
    "  - Text vectorization (e.g., TF-IDF, word embeddings).\n",
    "\n",
    "### **Feature Selection**\n",
    "- **Purpose**: Selects the most relevant features from the original dataset.\n",
    "- **Approach**: Removes redundant, irrelevant, or noisy features while keeping the important ones.\n",
    "- **Outcome**: Reduces the dimensionality of the dataset without altering the existing feature space.\n",
    "- **Examples**:\n",
    "  - Correlation analysis.\n",
    "  - Recursive Feature Elimination (RFE).\n",
    "  - Chi-square test.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Primary Goal\n",
    "\n",
    "| **Aspect**          | **Feature Extraction**                              | **Feature Selection**                            |\n",
    "|----------------------|----------------------------------------------------|-------------------------------------------------|\n",
    "| **Goal**            | Transform data into a new feature space.           | Identify and keep only the most relevant features. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Changes to Data\n",
    "\n",
    "| **Aspect**          | **Feature Extraction**                              | **Feature Selection**                            |\n",
    "|----------------------|----------------------------------------------------|-------------------------------------------------|\n",
    "| **Feature Space**   | Alters the original feature space (e.g., reduces or combines features). | Retains the original feature space. |\n",
    "| **Data Transformation** | Yes, involves transforming or combining features. | No, simply selects features without modification. |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Techniques Used\n",
    "\n",
    "| **Feature Extraction**                              | **Feature Selection**                            |\n",
    "|----------------------------------------------------|-------------------------------------------------|\n",
    "| Dimensionality reduction (e.g., PCA, t-SNE)         | Filter methods (e.g., correlation, Chi-square)   |\n",
    "| Signal processing techniques                        | Wrapper methods (e.g., Recursive Feature Elimination) |\n",
    "| Feature engineering (e.g., creating features using domain knowledge) | Embedded methods (e.g., Lasso regression)        |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Complexity\n",
    "\n",
    "- **Feature Extraction**: More computationally intensive as it involves creating or transforming features.\n",
    "- **Feature Selection**: Generally less computationally intensive since it works with existing features.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Use Cases\n",
    "\n",
    "### **Feature Extraction**\n",
    "- When raw data has complex relationships or is unstructured (e.g., text, images, audio).\n",
    "- When the goal is to reduce the dimensionality of the data for visualization or modeling.\n",
    "\n",
    "### **Feature Selection**\n",
    "- When the dataset contains irrelevant, redundant, or noisy features.\n",
    "- When interpretability is important (e.g., identifying which features are most important for predictions).\n",
    "\n",
    "---\n",
    "\n",
    "### Analogy\n",
    "\n",
    "- **Feature Extraction**: Like creating a summary or an abstract from a large document; you retain essential information in a new form.\n",
    "- **Feature Selection**: Like choosing the most relevant paragraphs from the document without rewriting or modifying the content.\n",
    "\n",
    "---\n",
    "\n",
    "### Example in Context\n",
    "\n",
    "### Dataset: A housing dataset with features like `area`, `number_of_rooms`, and `location`.\n",
    "\n",
    "- **Feature Extraction**: Create a new feature `price_per_sq_ft` by combining `price` and `area`.\n",
    "- **Feature Selection**: Use correlation to find that `number_of_rooms` is less correlated with house price compared to `area` and remove it.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "In practice, feature extraction and feature selection can complement each other to achieve the best model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad7221-b696-4cf8-8926-6359a8035895",
   "metadata": {},
   "source": [
    "## Encoding Methods in Machine Learning\n",
    "\n",
    "When dealing with categorical data, encoding methods are used to transform categories into numerical formats suitable for machine learning algorithms. Below are the most commonly used encoding methods:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. One-Hot Encoding**\n",
    "- **Description**: Converts each category into a binary vector.\n",
    "- **When to Use**: For **nominal data** (no inherent order) with a small number of categories.\n",
    "- **Example**: \n",
    "  - Input: `[\"Red\", \"Green\", \"Blue\"]`\n",
    "  - Output:\n",
    "    ```\n",
    "    Red  Green  Blue\n",
    "    1    0      0\n",
    "    0    1      0\n",
    "    0    0      1\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Label Encoding**\n",
    "- **Description**: Assigns a unique integer to each category.\n",
    "- **When to Use**: For **ordinal data** with a natural order or ranking.\n",
    "- **Example**: \n",
    "  - Input: `[\"Red\", \"Green\", \"Blue\"]`\n",
    "  - Output: `[0, 1, 2]`\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Target Encoding**\n",
    "- **Description**: Replaces each category with the mean of the target variable for that category.\n",
    "- **When to Use**: For regression or classification tasks where the relationship between the category and target is important.\n",
    "- **Example**: \n",
    "  - Input: `City = [\"A\", \"B\", \"C\"]`, Target: `[10, 20, 30]`\n",
    "  - Output: `City A: 10, City B: 20, City C: 30`\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Frequency Encoding**\n",
    "- **Description**: Replaces categories with their frequency of occurrence in the dataset.\n",
    "- **When to Use**: When frequency provides meaningful information.\n",
    "- **Example**: \n",
    "  - Input: `[\"A\", \"B\", \"B\", \"C\", \"A\", \"A\"]`\n",
    "  - Output: `[3, 2, 2, 1, 3, 3]`\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Binary Encoding**\n",
    "- **Description**: Combines label encoding and binary conversion. Each label is converted into an integer and then represented in binary.\n",
    "- **When to Use**: For **high-cardinality categorical data**.\n",
    "- **Example**:\n",
    "  - Input: `[\"A\", \"B\", \"C\", \"D\"]`\n",
    "  - Label Encoding: `[0, 1, 2, 3]`\n",
    "  - Binary Conversion:\n",
    "    ```\n",
    "    0 -> 0\n",
    "    1 -> 1\n",
    "    2 -> 10\n",
    "    3 -> 11\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Choosing the Right Encoding Method**\n",
    "- **One-Hot Encoding**: Best for nominal data with few categories.\n",
    "- **Label Encoding**: Ideal for ordinal data with a natural order.\n",
    "- **Target Encoding**: Effective in supervised learning tasks.\n",
    "- **Frequency/Binary Encoding**: Suitable for high-cardinality data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3060205",
   "metadata": {},
   "source": [
    "\n",
    "### One-Hot Encoding\n",
    "One-hot encoding is used to convert categorical variables into a binary matrix, making them usable in machine learning models.\n",
    "\n",
    "#### Example: Encoding a `Color` column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd87c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Color\n",
      "0    Red\n",
      "1   Blue\n",
      "2  Green\n",
      "3   Blue\n",
      "4    Red\n",
      "\n",
      "One-Hot Encoded DataFrame:\n",
      "   Color_Blue  Color_Green  Color_Red\n",
      "0       False        False       True\n",
      "1        True        False      False\n",
      "2       False         True      False\n",
      "3        True        False      False\n",
      "4       False        False       True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-hot encoding\n",
    "encoded_df = pd.get_dummies(df, columns=['Color'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nOne-Hot Encoded DataFrame:\")\n",
    "print(encoded_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa8240",
   "metadata": {},
   "source": [
    "\n",
    "### Creating New Features\n",
    "Domain expertise can be used to create meaningful features. For example:\n",
    "- Calculating a person's age based on their birth year.\n",
    "- Deriving a \"total price\" feature by multiplying quantity and unit price.\n",
    "\n",
    "#### Example: Creating `Total_Sales` from `Quantity` and `Unit_Price`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39b4e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Data with New Feature:\n",
      "   Quantity  Unit_Price  Total_Sales\n",
      "0        10          50          500\n",
      "1        20         100         2000\n",
      "2        30         200         6000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample dataset\n",
    "sales_data = {'Quantity': [10, 20, 30], 'Unit_Price': [50, 100, 200]}\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "\n",
    "# Creating a new feature\n",
    "sales_df['Total_Sales'] = sales_df['Quantity'] * sales_df['Unit_Price']\n",
    "\n",
    "print(\"Sales Data with New Feature:\")\n",
    "print(sales_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba36367",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Selection\n",
    "Correlation can help determine relationships between features and the target variable. Features with high correlation to the target are often useful.\n",
    "\n",
    "#### Example: Correlation Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685ab04-78d4-4b37-8f27-d1b8092b6f44",
   "metadata": {},
   "source": [
    "### **Dataset**\n",
    "Consider a small dataset with 3 features, `Feature1`, `Feature2`, and `Feature3`:\n",
    "\n",
    "| Feature1 | Feature2 | Feature3 | Target |\n",
    "|----------|----------|----------|--------|\n",
    "| 1        | 5        | 1        | 0      |\n",
    "| 2        | 4        | 2        | 1      |\n",
    "| 3        | 3        | 3        | 0      |\n",
    "| 4        | 2        | 4        | 1      |\n",
    "| 5        | 1        | 5        | 0      |\n",
    "\n",
    "We will compute the correlation between the features and remove features that are highly correlated with each other. Features that are highly correlated don't provide much new information, so they can be removed.\n",
    "\n",
    "### Step 1: Compute Correlation Between Features\n",
    "We can compute the correlation between each pair of features using a simple method. If the correlation value between two features is greater than 0.9 or less than -0.9, we will remove one of them.\n",
    "\n",
    "### Code Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c572435-759e-49f9-ae9a-822e5377e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix:\n",
      "          Feature1  Feature2  Feature3  Target\n",
      "Feature1       1.0      -1.0       1.0     0.0\n",
      "Feature2      -1.0       1.0      -1.0     0.0\n",
      "Feature3       1.0      -1.0       1.0     0.0\n",
      "Target         0.0       0.0       0.0     1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    \"Feature1\": [1, 2, 3, 4, 5],\n",
    "    \"Feature2\": [5, 4, 3, 2, 1],\n",
    "    \"Feature3\": [1, 2, 3, 4, 5],\n",
    "    \"Target\": [0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Display correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# If two features are highly correlated (correlation > 0.9 or < -0.9), we will drop one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc4839-bcc9-48ae-93dd-0ace6e8fbc85",
   "metadata": {},
   "source": [
    "### Step 2: Select Features\n",
    "From the correlation matrix, we can see that `Feature1`, `Feature2`, and `Feature3` are highly correlated with each other. If we remove one of them, the model will still perform well with fewer features.\n",
    "\n",
    "For example, we can remove either `Feature2` or `Feature3`, as they have a perfect negative correlation with `Feature1`. Let's remove `Feature2` and keep `Feature1` and `Feature3`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2db07e-4b5e-4dbc-ad8d-48584f0de4a1",
   "metadata": {},
   "source": [
    "### Step 3: Drop the Highly Correlated Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3bea37e-e414-4b07-a0b6-aeb262b3fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after Feature Selection:\n",
      "   Feature1  Feature3  Target\n",
      "0         1         1       0\n",
      "1         2         2       1\n",
      "2         3         3       0\n",
      "3         4         4       1\n",
      "4         5         5       0\n"
     ]
    }
   ],
   "source": [
    "# Remove Feature2 because it's highly correlated with Feature1 and Feature3\n",
    "df_selected = df.drop('Feature2', axis=1)\n",
    "\n",
    "print(\"Dataset after Feature Selection:\")\n",
    "print(df_selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65398e9",
   "metadata": {},
   "source": [
    "## Practice Questions: Feature Extraction and Selection\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Extraction: Handling Categorical Data with One-Hot Encoding**\n",
    "- **Task:** \n",
    "  - You are given a dataset with a categorical column `Color` containing values `Red`, `Blue`, and `Green`.\n",
    "  - Apply one-hot encoding to the `Color` column and add it as new columns to the dataset.\n",
    "  \n",
    "  **Dataset Example:**\n",
    "  ```python\n",
    "  data = {'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']}\n",
    "  ```\n",
    "\n",
    "\n",
    "### Feature Extraction: Normalizing Numerical Data\n",
    "\n",
    "#### Task:\n",
    "Given the dataset below, normalize the numerical features **Age** and **Salary** using Min-Max scaling.\n",
    "\n",
    "**Dataset Example:**\n",
    "```python\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "        'Age': [25, 30, 35, 40, 45],\n",
    "        'Salary': [50000, 60000, 70000, 80000, 90000]}\n",
    "\n",
    "# Min-Max scaling function\n",
    "def min_max_scaling(column):\n",
    "    min_value = min(column)\n",
    "    max_value = max(column)\n",
    "    return [(x - min_value) / (max_value - min_value) for x in column]\n",
    " ```\n",
    "\n",
    "\n",
    "### Feature Selection: Removing Highly Correlated Features\n",
    "- **Task:** \n",
    "  - You are given a dataset with three features, **Feature1**, **Feature2**, and **Feature3**. Compute the correlation matrix and drop one feature that is highly correlated with the others (correlation > 0.9 or < -0.9).\n",
    "  \n",
    "- **Dataset Example:**\n",
    "  ```python\n",
    "  data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "          'Feature2': [5, 4, 3, 2, 1],\n",
    "          'Feature3': [1, 2, 3, 4, 5],\n",
    "          'Target': [0, 1, 0, 1, 0]}\n",
    "\n",
    "\n",
    "### Feature Extraction: Creating New Features from Existing Data**\n",
    "- **Task:** \n",
    "  - Given the dataset, create a new feature **AgeGroup** based on the **Age** column, where:\n",
    "    - If **Age** is less than 30, the **AgeGroup** is \"Young\".\n",
    "    - If **Age** is between 30 and 50 (inclusive), the **AgeGroup** is \"Adult\".\n",
    "    - If **Age** is greater than 50, the **AgeGroup** is \"Senior\".\n",
    "  \n",
    "- **Dataset Example:**\n",
    "  ```python\n",
    "  data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "          'Age': [25, 30, 35, 40, 55]}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
